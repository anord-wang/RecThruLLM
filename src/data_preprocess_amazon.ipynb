{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook Processes the Public Amazon Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import lil_matrix, save_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    We follow the Multi-VAE paper\n",
    "    and fix the random seed as 98765\n",
    "'''\n",
    "seed = 98765\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def save_text(data, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(data)\n",
    "\n",
    "def save_pickle(data, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def ReadLineFromFile(path):\n",
    "    lines = []\n",
    "    with open(path,'r') as fd:\n",
    "        for line in fd:\n",
    "            lines.append(line.rstrip('\\n'))\n",
    "    return lines\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "        \n",
    "def add_comma(num):\n",
    "    # 1000000 -> 1,000,000\n",
    "    str_num = str(num)\n",
    "    res_num = ''\n",
    "    for i in range(len(str_num)):\n",
    "        res_num += str_num[i]\n",
    "        if (len(str_num)-i-1) % 3 == 0:\n",
    "            res_num += ','\n",
    "    return res_num[:-1]\n",
    "\n",
    "def dict_to_txt(dict_data, layer=1, recursive=True):\n",
    "    txt = \"{\\n\"\n",
    "    for k,v in dict_data.items():\n",
    "        txt += \" \"*2*layer + \"{}:{}\\n\".format(\n",
    "            dict_to_txt(k, layer=layer+1, recursive=recursive) if isinstance(k, dict) else k, \n",
    "            dict_to_txt(v, layer=layer+1, recursive=recursive) if isinstance(v, dict) else v)\n",
    "    txt += \" \"*(layer-1)*2 + \"}\"\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Select the Target Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_data_name = 'sports'\n",
    "if not os.path.exists(short_data_name):\n",
    "    os.mkdir(short_data_name)\n",
    "    \n",
    "if short_data_name == 'beauty':\n",
    "    full_data_name = 'Beauty'\n",
    "elif short_data_name == 'toys':\n",
    "    full_data_name = 'Toys_and_Games'\n",
    "elif short_data_name == 'sports':\n",
    "    full_data_name = 'Sports_and_Outdoors'\n",
    "else:\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Functions to Extract Interaction Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Amazon(dataset_name, rating_score):\n",
    "    '''\n",
    "        reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
    "        asin - (Amazon Standard Identification Number) \n",
    "             -  ID of the product, e.g. 0000013714\n",
    "        reviewerName - name of the reviewer\n",
    "        helpful - helpfulness rating of the review, e.g. 2/3\n",
    "            --\"helpful\": [2, 3],\n",
    "        reviewText - text of the review\n",
    "            --\"reviewText\": \"I bought this for my husband who plays the piano. ...\"\n",
    "        overall - rating of the product\n",
    "            --\"overall\": 5.0,\n",
    "        summary - summary of the review\n",
    "            --\"summary\": \"Heavenly Highway Hymns\",\n",
    "        unixReviewTime - time of the review (unix time)\n",
    "            --\"unixReviewTime\": 1252800000,\n",
    "        reviewTime - time of the review (raw)\n",
    "            --\"reviewTime\": \"09 13, 2009\"\n",
    "    '''\n",
    "    datas = []\n",
    "    data_file = './raw_data/reviews_' + dataset_name + '.json.gz'\n",
    "    for record in parse(data_file):\n",
    "        if float(record['overall']) <= rating_score:\n",
    "            continue\n",
    "        user = record['reviewerID']\n",
    "        item = record['asin']\n",
    "        time = record['unixReviewTime']\n",
    "        datas.append((user, item, int(time)))\n",
    "    return datas\n",
    "\n",
    "def get_interaction(datas):\n",
    "    user_seq = {}\n",
    "    for data in datas:\n",
    "        user, item, time = data\n",
    "        if user in user_seq:\n",
    "            user_seq[user].append((item, time))\n",
    "        else:\n",
    "            user_seq[user] = []\n",
    "            user_seq[user].append((item, time))\n",
    "\n",
    "    for user, item_time in user_seq.items():\n",
    "        item_time.sort(key=lambda x: x[1]) \n",
    "        items = []\n",
    "        for t in item_time:\n",
    "            items.append(t[0])\n",
    "        user_seq[user] = items\n",
    "    return user_seq\n",
    "\n",
    "# K-core user_core item_core\n",
    "def check_Kcore(user_items, user_core, item_core):\n",
    "    user_count = defaultdict(int)\n",
    "    item_count = defaultdict(int)\n",
    "    for user, items in user_items.items():\n",
    "        for item in items:\n",
    "            user_count[user] += 1\n",
    "            item_count[item] += 1\n",
    "\n",
    "    for user, num in user_count.items():\n",
    "        if num < user_core:\n",
    "            return user_count, item_count, False\n",
    "    for item, num in item_count.items():\n",
    "        if num < item_core:\n",
    "            return user_count, item_count, False\n",
    "    return user_count, item_count, True \n",
    "\n",
    "\n",
    "# filter the K-core\n",
    "def filter_Kcore(user_items, user_core, item_core):\n",
    "    user_count, item_count, isKcore = check_Kcore(user_items, user_core, item_core)\n",
    "    while not isKcore:\n",
    "        for user, num in user_count.items():\n",
    "            if user_count[user] < user_core:\n",
    "                user_items.pop(user)\n",
    "            else:\n",
    "                for item in user_items[user]:\n",
    "                    if item_count[item] < item_core:\n",
    "                        user_items[user].remove(item)\n",
    "        user_count, item_count, isKcore = check_Kcore(user_items, user_core, item_core)\n",
    "    return user_items\n",
    "\n",
    "def id_map(user_items): # user_items dict\n",
    "    user2id = {} # raw 2 uid\n",
    "    item2id = {} # raw 2 iid\n",
    "    id2user = {} # uid 2 raw\n",
    "    id2item = {} # iid 2 raw\n",
    "    user_id = 0\n",
    "    item_id = 0\n",
    "    final_data = {}\n",
    "    random_user_list = list(user_items.keys())\n",
    "    random.shuffle(random_user_list)\n",
    "    for user in random_user_list:\n",
    "        items = user_items[user]\n",
    "        if user not in user2id:\n",
    "            user2id[user] = user_id\n",
    "            id2user[user_id] = user\n",
    "            user_id += 1\n",
    "        iids = [] # item id lists\n",
    "        for item in items:\n",
    "            if item not in item2id:\n",
    "                item2id[item] = item_id\n",
    "                id2item[item_id] = item\n",
    "                item_id += 1\n",
    "            iids.append(item2id[item])\n",
    "        uid = user2id[user]\n",
    "        final_data[uid] = iids\n",
    "    data_maps = {\n",
    "        'user2id': user2id,\n",
    "        'item2id': item2id,\n",
    "        'id2user': id2user,\n",
    "        'id2item': id2item\n",
    "    }\n",
    "    return final_data, user_id-1, item_id-1, data_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Functions to Extract Content Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Amazon_meta(dataset_name, data_maps):\n",
    "    '''\n",
    "        asin - ID of the product, e.g. 0000031852\n",
    "            --\"asin\": \"0000031852\",\n",
    "        title - name of the product\n",
    "            --\"title\": \"Girls Ballet Tutu Zebra Hot Pink\",\n",
    "        description\n",
    "        price - price in US dollars (at time of crawl)\n",
    "            --\"price\": 3.17,\n",
    "        imUrl - url of the product image (str)\n",
    "            --\"imUrl\": \"http://ecx.images-amazon.com/images/I/51fAmVkTbyL._SY300_.jpg\",\n",
    "        related - related products (also bought, also viewed, bought together, buy after viewing)\n",
    "            --\"related\":{\n",
    "                \"also_bought\": [\"B00JHONN1S\"],\n",
    "                \"also_viewed\": [\"B002BZX8Z6\"],\n",
    "                \"bought_together\": [\"B002BZX8Z6\"]\n",
    "            },\n",
    "        salesRank - sales rank information\n",
    "            --\"salesRank\": {\"Toys & Games\": 211836}\n",
    "        brand - brand name\n",
    "            --\"brand\": \"Coxlures\",\n",
    "        categories - list of categories the product belongs to\n",
    "            --\"categories\": [[\"Sports & Outdoors\", \"Other Sports\", \"Dance\"]]\n",
    "    '''\n",
    "    datas = {}\n",
    "    meta_file = './raw_data/meta_' + dataset_name + '.json.gz'\n",
    "    item_asins = list(data_maps['item2id'].keys())\n",
    "    for info in parse(meta_file):\n",
    "        if info['asin'] not in item_asins:\n",
    "            continue\n",
    "        datas[info['asin']] = info\n",
    "    return datas\n",
    "\n",
    "# categories and brand is all attribute\n",
    "def get_attr_Amazon(meta_infos, datamaps, attr_core):\n",
    "    # First, calculate the number of different attributes\n",
    "    attrs = defaultdict(int)\n",
    "    for vid, info in tqdm(meta_infos.items()):\n",
    "        for cates in info['categories']:\n",
    "            for cate in cates[1:]:\n",
    "                attrs[cate] +=1\n",
    "        try:\n",
    "            attrs[info['brand']] += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # We only save attributes that appear more than attr_core times\n",
    "    print(f'before delete, attribute num:{len(attrs)}')\n",
    "    new_meta = {}\n",
    "    for vid, info in tqdm(meta_infos.items()):\n",
    "        new_meta[vid] = []\n",
    "        try:\n",
    "            if attrs[info['brand']] >= attr_core:\n",
    "                new_meta[vid].append(info['brand'])\n",
    "        except:\n",
    "            pass\n",
    "        for cates in info['categories']:\n",
    "            for cate in cates[1:]:\n",
    "                if attrs[cate] >= attr_core:\n",
    "                    new_meta[vid].append(cate)\n",
    "    \n",
    "    # Save the attribute data\n",
    "    attr2id = {}\n",
    "    id2attr = {}\n",
    "    attrid2num = defaultdict(int)\n",
    "    attr_id = 1\n",
    "    items2attrs = {}\n",
    "    attr_lens = []\n",
    "\n",
    "    for vid, attrs in new_meta.items():\n",
    "        item_id = datamaps['item2id'][vid]\n",
    "        items2attrs[item_id] = []\n",
    "        for attr in attrs:\n",
    "            if attr not in attr2id:\n",
    "                attr2id[attr] = attr_id\n",
    "                id2attr[attr_id] = attr\n",
    "                attr_id += 1\n",
    "            attrid2num[attr2id[attr]] += 1\n",
    "            items2attrs[item_id].append(attr2id[attr])\n",
    "        attr_lens.append(len(items2attrs[item_id]))\n",
    "        \n",
    "    print(f'before delete, attribute num:{len(attr2id)}')\n",
    "    print(f'attributes len, Min:{np.min(attr_lens)}, Max:{np.max(attr_lens)}, Avg.:{np.mean(attr_lens):.4f}')\n",
    "    # update datamap\n",
    "    datamaps['attr2id'] = attr2id\n",
    "    datamaps['id2attr'] = id2attr\n",
    "    datamaps['attrid2num'] = attrid2num\n",
    "    return len(attr2id), np.mean(attr_lens), datamaps, items2attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Processing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data_name, acronym, data_type='Amazon'):\n",
    "    assert data_type in {'Amazon', 'Yelp'}\n",
    "    rating_score = 3.0  # rating score smaller than this score would be deleted\n",
    "    # user 5-core item 5-core\n",
    "    user_core = 5\n",
    "    item_core = 5\n",
    "    attr_core = 0\n",
    "\n",
    "    if data_type == 'Yelp':\n",
    "        date_max = '2019-12-31 00:00:00'\n",
    "        date_min = '2019-01-01 00:00:00'\n",
    "        datas = Yelp(date_min, date_max, rating_score)\n",
    "    else:\n",
    "        datas = Amazon(data_name+'_5', rating_score=rating_score)\n",
    "\n",
    "    user_items = get_interaction(datas)\n",
    "    print(f'{data_name} Raw data has been processed! Lower than {rating_score} are deleted!')\n",
    "    # raw_id user: [item1, item2, item3...]\n",
    "    user_items = filter_Kcore(user_items, user_core=user_core, item_core=item_core)\n",
    "    print(f'User {user_core}-core complete! Item {item_core}-core complete!')\n",
    "\n",
    "    user_items, user_num, item_num, data_maps = id_map(user_items)\n",
    "    user_count, item_count, _ = check_Kcore(user_items, user_core=user_core, item_core=item_core)\n",
    "    user_count_list = list(user_count.values())\n",
    "    user_avg, user_min, user_max = np.mean(user_count_list), np.min(user_count_list), np.max(user_count_list)\n",
    "    item_count_list = list(item_count.values())\n",
    "    item_avg, item_min, item_max = np.mean(item_count_list), np.min(item_count_list), np.max(item_count_list)\n",
    "    interact_num = np.sum([x for x in user_count_list])\n",
    "    sparsity = (1 - interact_num / (user_num * item_num)) * 100\n",
    "    show_info = f'Total User: {user_num}, Avg User: {user_avg:.4f}, Min Len: {user_min}, Max Len: {user_max}\\n' + \\\n",
    "                f'Total Item: {item_num}, Avg Item: {item_avg:.4f}, Min Inter: {item_min}, Max Inter: {item_max}\\n' + \\\n",
    "                f'Iteraction Num: {interact_num}, Sparsity: {sparsity:.2f}%'\n",
    "    print(show_info)\n",
    "\n",
    "    print('Begin extracting meta infos...')\n",
    "\n",
    "    meta_infos = Amazon_meta(data_name, data_maps)\n",
    "    attr_num, avg_attr, datamaps, item2attrs = get_attr_Amazon(meta_infos, data_maps, attr_core)\n",
    "\n",
    "    print(f'{data_name} & {add_comma(user_num)}& {add_comma(item_num)} & {user_avg:.1f}'\n",
    "          f'& {item_avg:.1f}& {add_comma(interact_num)}& {sparsity:.2f}\\%&{add_comma(attr_num)}&'\n",
    "          f'{avg_attr:.1f} \\\\')\n",
    "\n",
    "    return meta_infos, user_items, item2attrs, datamaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sports_and_Outdoors Raw data has been processed! Lower than 3.0 are deleted!\n",
      "User 5-core complete! Item 5-core complete!\n",
      "Total User: 22685, Avg User: 8.1865, Min Len: 5, Max Len: 261\n",
      "Total Item: 12300, Avg Item: 15.0978, Min Inter: 5, Max Inter: 840\n",
      "Iteraction Num: 185718, Sparsity: 99.93%\n",
      "Begin extracting meta infos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 12301/12301 [00:00<00:00, 1136659.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before delete, attribute num:2963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 12301/12301 [00:00<00:00, 815189.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before delete, attribute num:2963\n",
      "attributes len, Min:0, Max:29, Avg.:4.5559\n",
      "Sports_and_Outdoors & 22,685& 12,300 & 8.2& 15.1& 185,718& 99.93\\%&2,963&4.6 \\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "meta_infos, user_items, item2attrs, datamaps = main(full_data_name, short_data_name, data_type='Amazon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Processing the Interaction Data and Save as Sparse Matrices\n",
    "\n",
    "For each user, we use 80% of the historical interactions as the training data, 10% as the validation data, and another 10% as the testing data.   \n",
    "Please note that at least one item is used for validation, and at least one another item is used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_user_item_interactions(interactions):\n",
    "    # Determine the total number of users and items\n",
    "    num_users = len(interactions)\n",
    "    num_items = max([max(items) for items in interactions.values()])+1\n",
    "\n",
    "    # Create empty dictionaries for training, validation, and testing interactions\n",
    "    train_interactions = {}\n",
    "    val_interactions = {}\n",
    "    test_interactions = {}\n",
    "\n",
    "    # Iterate over each user\n",
    "    for user_id, item_list in interactions.items():\n",
    "        # Shuffle the list of item interactions for each user\n",
    "        np.random.shuffle(item_list)\n",
    "\n",
    "        # Calculate the number of interactions for training, validation, and testing\n",
    "        # We keep at least one validation/testing item for each user\n",
    "        num_val = max(1, int(len(item_list) * 0.1))\n",
    "        num_test = num_val\n",
    "        num_train = int(len(item_list)) - num_val - num_test\n",
    "\n",
    "        # Split the shuffled item interactions list\n",
    "        train_items = item_list[:num_train]\n",
    "        val_items = item_list[num_train:num_train + num_val]\n",
    "        test_items = item_list[num_train + num_val:num_train + num_val + num_test]\n",
    "\n",
    "        # Assign the interactions to the corresponding datasets\n",
    "        train_interactions[user_id] = train_items\n",
    "        val_interactions[user_id] = val_items\n",
    "        test_interactions[user_id] = test_items\n",
    "\n",
    "    # Convert dictionaries into sparse matrices\n",
    "    train_matrix = dict_to_sparse_matrix(train_interactions, num_users, num_items)\n",
    "    val_matrix = dict_to_sparse_matrix(val_interactions, num_users, num_items)\n",
    "    test_matrix = dict_to_sparse_matrix(test_interactions, num_users, num_items)\n",
    "\n",
    "    return train_matrix, val_matrix, test_matrix\n",
    "\n",
    "\n",
    "def dict_to_sparse_matrix(dictionary, num_rows, num_cols):\n",
    "    matrix = lil_matrix((num_rows, num_cols), dtype=np.float32)\n",
    "    for row, items in dictionary.items():\n",
    "        for col in items:\n",
    "            matrix[row, col] = 1.0  \n",
    "    return matrix.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix, val_matrix, test_matrix = split_user_item_interactions(user_items)\n",
    "data_root = os.path.join(\"dataset\", short_data_name)\n",
    "if not os.path.exists(data_root):\n",
    "    os.makedirs(data_root)\n",
    "save_npz(os.path.join(data_root, 'train_matrix.npz'), train_matrix)\n",
    "save_npz(os.path.join(data_root, 'val_matrix.npz'), val_matrix)\n",
    "save_npz(os.path.join(data_root, 'test_matrix.npz'), test_matrix)\n",
    "\n",
    "num_users, num_items = train_matrix.shape\n",
    "meta_data = {\"num_users\":num_users, \"num_items\":num_items}\n",
    "meta_txt = f\"num_users:{num_users}\\nnum_items:{num_items}\"\n",
    "\n",
    "meta_path = os.path.join(data_root, \"meta.pkl\")\n",
    "meta_txt_path = os.path.join(data_root, \"meta.txt\")\n",
    "\n",
    "with open(meta_path, \"wb\") as file:\n",
    "    pickle.dump(meta_data, file)\n",
    "        \n",
    "with open(meta_txt_path, \"w\") as file:\n",
    "    file.write(meta_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Processing the Item-Specific Textual Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The interested texts associated with the items\n",
    "item_texts = [\"title\", \"brand\", \"categories\", \"description\"]\n",
    "item_texts = {item_text:[] for item_text in item_texts}\n",
    "\n",
    "# Process items and append to respective lists\n",
    "for asin, properties in sorted(meta_infos.items(), key=lambda x: datamaps[\"item2id\"].get(x[0], 0)):\n",
    "    if asin in datamaps[\"item2id\"]:\n",
    "        item_id = datamaps[\"item2id\"][asin]\n",
    "        \n",
    "        # Obtain the title of item_id\n",
    "        title = properties.get(\"title\")\n",
    "        if title:\n",
    "            item_texts[\"title\"].append([f\"The title of item_{item_id} is:\", f\" {title}\"])\n",
    "        \n",
    "        # Obtain the brand of item_id\n",
    "        brand = properties.get(\"brand\")\n",
    "        if brand:\n",
    "            item_texts[\"brand\"].append([f\"The brand of item_{item_id} is:\", f\" {brand}\"])\n",
    "        \n",
    "        # Obtain the categories of item_id\n",
    "        categories = properties.get(\"categories\")\n",
    "        if categories:\n",
    "            categories_text = \", \".join(categories[0])\n",
    "            item_texts[\"categories\"].append([f\"The categories of item_{item_id} are:\", f\" {categories_text}\"])\n",
    "        \n",
    "        # Obtain the description of item_id\n",
    "        description = properties.get(\"description\")\n",
    "        if description:\n",
    "            item_texts[\"description\"].append([f\"The description of item_{item_id} is:\", f\" {description}\"])\n",
    "\n",
    "# Save output lists to files\n",
    "item_text_root = os.path.join(data_root, \"item_texts\")\n",
    "if not os.path.exists(item_text_root):\n",
    "    os.makedirs(item_text_root)\n",
    "\n",
    "# Save output lists to files\n",
    "for name, item_text in item_texts.items():\n",
    "    # For human to read\n",
    "    text_filepath = os.path.join(item_text_root, f\"{name}.txt\")\n",
    "    # For machine to read\n",
    "    pkl_filepath = os.path.join(item_text_root, f\"{name}.pkl\")\n",
    "    \n",
    "    with open(text_filepath, \"w\") as file:\n",
    "        file.write(\"\\n\".join(\n",
    "            [\"\".join([prompt, main]) for prompt, main in item_text]\n",
    "        ))\n",
    "        \n",
    "    with open(pkl_filepath, \"wb\") as file:\n",
    "         pickle.dump(item_text, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Processing the Textual Data Associated with a User/Item Pair\n",
    "\n",
    "#### 5.3.1 Processing the Review Data of user_i to item_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data = []\n",
    "for review in parse(\"./raw_data/reviews_{}_5.json.gz\".format(full_data_name)):\n",
    "    review_data.append(review)\n",
    "\n",
    "# Convert review list to dictionary\n",
    "review_dict = {}\n",
    "for review in review_data:\n",
    "    reviewer_id = review['reviewerID']\n",
    "    asin = review['asin']\n",
    "    review_dict[(reviewer_id, asin)] = review['reviewText']\n",
    "\n",
    "# Traverse the sparse matrix and retrieve review texts efficiently\n",
    "reviews = []\n",
    "for user_id, item_id in zip(train_matrix.nonzero()[0], train_matrix.nonzero()[1]):\n",
    "    reviewer_id = datamaps[\"id2user\"].get(user_id)\n",
    "    asin = datamaps[\"id2item\"].get(item_id)\n",
    "    if reviewer_id and asin:\n",
    "        review_text = review_dict.get((reviewer_id, asin))\n",
    "        if review_text:\n",
    "            reviews.append([f\"user_{user_id} wrote the following review for item_{item_id}:\", f\" {review_text}\"])\n",
    "            \n",
    "# Save output lists to files\n",
    "user_item_text_root = os.path.join(data_root, \"user_item_texts\")\n",
    "if not os.path.exists(user_item_text_root):\n",
    "    os.makedirs(user_item_text_root)\n",
    "\n",
    "# Save output lists to files\n",
    "text_filepath = os.path.join(user_item_text_root, \"review.txt\")\n",
    "pkl_filepath = os.path.join(user_item_text_root, \"review.pkl\")\n",
    "\n",
    "with open(text_filepath, \"w\") as file:\n",
    "    file.write(\"\\n\".join(\n",
    "        [\"\".join([prompt, main]) for prompt, main in reviews]\n",
    "    ))\n",
    "\n",
    "with open(pkl_filepath, \"wb\") as file:\n",
    "     pickle.dump(reviews, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 Processing the Explanation Data of user_i to item_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_data = load_pickle('./raw_data/reviews_{}.pickle'.format(full_data_name))\n",
    "\n",
    "# Convert explain list to dictionary\n",
    "explain_dict = {}\n",
    "for explain in explain_data:\n",
    "    reviewer_id = explain['user']\n",
    "    asin = explain['item']\n",
    "    explain_dict[(reviewer_id, asin)] = explain['text']\n",
    "\n",
    "# Traverse the sparse matrix and retrieve explain texts efficiently\n",
    "explains = []\n",
    "for user_id, item_id in zip(train_matrix.nonzero()[0], train_matrix.nonzero()[1]):\n",
    "    reviewer_id = datamaps[\"id2user\"].get(user_id)\n",
    "    asin = datamaps[\"id2item\"].get(item_id)\n",
    "    if reviewer_id and asin:\n",
    "        explain_text = explain_dict.get((reviewer_id, asin))\n",
    "        if explain_text:\n",
    "            explains.append([f\"user_{user_id} explains the reason for purchasing item_{item_id}:\", f\" {explain_text}\"])\n",
    "\n",
    "# Save output lists to files\n",
    "text_filepath = os.path.join(user_item_text_root, \"explain.txt\")\n",
    "pkl_filepath = os.path.join(user_item_text_root, \"explain.pkl\")\n",
    "\n",
    "with open(text_filepath, \"w\") as file:\n",
    "    file.write(\"\\n\".join(\n",
    "        [\"\".join([prompt, main]) for prompt, main in explains]\n",
    "    ))\n",
    "\n",
    "with open(pkl_filepath, \"wb\") as file:\n",
    "     pickle.dump(explains, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
